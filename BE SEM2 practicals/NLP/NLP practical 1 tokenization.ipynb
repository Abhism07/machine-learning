{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e3b7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a929a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"All Indians are ..my brothers and I don't have sisters. I love my country and I am proud of its rich and varied heritage. I shall strive to be worthy !~!!of it. I --shall respect my parents, teachers and all elders and treat everyone with courtesyğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ ğŸ˜€ğŸ˜€ğŸ˜ƒğŸ˜ƒğŸ¤£ğŸ¤£.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63b634e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"All Indians are ..my brothers and I don't have sisters. I love my country and I am proud of its rich and varied heritage. I shall strive to be worthy !~!!of it. I --shall respect my parents, teachers and all elders and treat everyone with courtesyğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜ƒğŸ˜ƒğŸ¤£ğŸ¤£.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74df76",
   "metadata": {},
   "source": [
    "# TOKENIZATION\n",
    "Tokenization is the first step in any NLP pipeline. It has an important effect on the rest of your pipeline. A tokenizer breaks unstructured data and natural language text into chunks of information that can be considered as discrete elements. The token occurrences in a document can be used directly as a vector representing that document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f29d7",
   "metadata": {},
   "source": [
    "# White Space Tokenization\n",
    "\n",
    "The simplest way to tokenize text is to use whitespace within a string as the â€œdelimiterâ€ of words. This can be accomplished with Pythonâ€™s split function, which is available on all string object instances as well as on the string built-in class itself. You can change the separator any way you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b57db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'Indians', 'are', '..my', 'brothers', 'and', 'I', \"don't\", 'have', 'sisters.', 'I', 'love', 'my', 'country', 'and', 'I', 'am', 'proud', 'of', 'its', 'rich', 'and', 'varied', 'heritage.', 'I', 'shall', 'strive', 'to', 'be', 'worthy', '!~!!of', 'it.', 'I', '--shall', 'respect', 'my', 'parents,', 'teachers', 'and', 'all', 'elders', 'and', 'treat', 'everyone', 'with', 'courtesyğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜ƒğŸ˜ƒğŸ¤£ğŸ¤£.'] \n",
      "length of tokens: 46\n"
     ]
    }
   ],
   "source": [
    "whitespace_token = WhitespaceTokenizer().tokenize(sent)\n",
    "print(whitespace_token,\"\\nlength of tokens:\",len(whitespace_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152df16",
   "metadata": {},
   "source": [
    "# Punctuation-based tokenizer\n",
    "This tokenizer splits the sentences into words based on whitespaces and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd4d5b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'Indians', 'are', '..', 'my', 'brothers', 'and', 'I', 'don', \"'\", 't', 'have', 'sisters', '.', 'I', 'love', 'my', 'country', 'and', 'I', 'am', 'proud', 'of', 'its', 'rich', 'and', 'varied', 'heritage', '.', 'I', 'shall', 'strive', 'to', 'be', 'worthy', '!~!!', 'of', 'it', '.', 'I', '--', 'shall', 'respect', 'my', 'parents', ',', 'teachers', 'and', 'all', 'elders', 'and', 'treat', 'everyone', 'with', 'courtesy', 'ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜ƒğŸ˜ƒğŸ¤£ğŸ¤£.'] \n",
      " Length of tokens: 56\n"
     ]
    }
   ],
   "source": [
    "punct_based =WordPunctTokenizer().tokenize(sent)\n",
    "print(punct_based,\"\\n Length of tokens:\",len(punct_based))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a2f00",
   "metadata": {},
   "source": [
    "# Treebank Word tokenizer\n",
    "\n",
    "This tokenizer incorporates a variety of common rules for english word tokenization. It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains decimal numbers as a single token. Besides, it contains rules for English contractions. \n",
    "\n",
    "For example â€œdonâ€™tâ€ is tokenized as [â€œdoâ€, â€œnâ€™tâ€]. You can find all the rules for the Treebank Tokenizer at this link. https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5563abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'Indians', 'are', '..my', 'brothers', 'and', 'I', 'do', \"n't\", 'have', 'sisters.', 'I', 'love', 'my', 'country', 'and', 'I', 'am', 'proud', 'of', 'its', 'rich', 'and', 'varied', 'heritage.', 'I', 'shall', 'strive', 'to', 'be', 'worthy', '!', '~', '!', '!', 'of', 'it.', 'I', '--', 'shall', 'respect', 'my', 'parents', ',', 'teachers', 'and', 'all', 'elders', 'and', 'treat', 'everyone', 'with', 'courtesyğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜ƒğŸ˜ƒğŸ¤£ğŸ¤£', '.'] \n",
      "Length of tokens: 54\n"
     ]
    }
   ],
   "source": [
    "tree_bank =TreebankWordTokenizer().tokenize(sent)\n",
    "print(tree_bank,\"\\nLength of tokens:\",len(tree_bank))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4dbae5",
   "metadata": {},
   "source": [
    "# Tweet tokenizer\n",
    "\n",
    "When we want to apply tokenization in text data like tweets, the tokenizers mentioned above canâ€™t produce practical tokens. Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different words if we need them for tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ad056d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'Indians', 'are', '..', 'my', 'brothers', 'and', 'I', \"don't\", 'have', 'sisters', '.', 'I', 'love', 'my', 'country', 'and', 'I', 'am', 'proud', 'of', 'its', 'rich', 'and', 'varied', 'heritage', '.', 'I', 'shall', 'strive', 'to', 'be', 'worthy', '!', '~', '!', '!', 'of', 'it', '.', 'I', '-', '-', 'shall', 'respect', 'my', 'parents', ',', 'teachers', 'and', 'all', 'elders', 'and', 'treat', 'everyone', 'with', 'courtesy', 'ğŸ˜€', 'ğŸ˜€', 'ğŸ˜€', 'ğŸ˜ƒ', 'ğŸ˜ƒ', 'ğŸ¤£', 'ğŸ¤£', '.'] \n",
      "length of tokens: 65\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweet = TweetTokenizer().tokenize(sent)\n",
    "print(tweet,\"\\nlength of tokens:\",len(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f5852",
   "metadata": {},
   "source": [
    "# MWET tokenizer\n",
    "\n",
    "NLTKâ€™s multi-word expression tokenizer (MWETokenizer) provides a function add_mwe() that allows the user to enter multiple word expressions before using the tokenizer on the text. More simply, it can merge multi-word expressions into single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9fbdab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Hunger_Games', 'film', 'series', 'is', 'composed', 'of', 'science', 'fiction', 'dystopian', 'adventure', 'films,', 'based', 'on', 'The', 'Hunger_Games', 'trilogy', 'of', 'novels', 'by', 'American', 'author', 'Suzanne', 'Collins.'] \n",
      "Length of tokens: 24\n"
     ]
    }
   ],
   "source": [
    "sent1= \"The Hunger Games film series is composed of science fiction dystopian adventure films, based on The Hunger Games trilogy of novels by American author Suzanne Collins.\"\n",
    "mwe=MWETokenizer()\n",
    "mwe.add_mwe(('Hunger','Games'))\n",
    "mwe1=mwe.tokenize(sent1.split())\n",
    "print(mwe1,\"\\nLength of tokens:\",len(mwe1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e071acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
